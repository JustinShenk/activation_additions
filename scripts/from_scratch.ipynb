{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # AVE From Scratch\n",
    "\n",
    " Let's reproduce the top example from the AVE post, from scratch in [TransformerLens](https://github.com/neelnanda-io/TransformerLens)!\n",
    " Reading this is a good way to understand the internals of the library, and tinker with the method in a low-friciton way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Dict, Union, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)  # save memory\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-xl\")\n",
    "model.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Settings from qualitative notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "\n",
    "# Specific to the love/hate example\n",
    "prompt_add, prompt_sub = \"Love\", \"Hate\"\n",
    "coeff = 5\n",
    "act_name = 6\n",
    "prompt = \"I hate you because\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Padding\n",
    " We're taking the difference between Love & Hate residual streams, but we run into trouble because `Love` is a single token, whereas `Hate` is two tokens (`H`, `ate`). We solve this by right-padding `Love` with spaces until it's the same length as `Hate`. I've done this generically below, but conceptually it isn't important.\n",
    "\n",
    " (PS: We tried padding by model.tokenizer.eos_token and got worse results compared to spaces. We don't know why this is yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Love ' 'Hate'\n"
     ]
    }
   ],
   "source": [
    "tlen = lambda prompt: model.to_tokens(prompt).shape[1]\n",
    "pad_right = lambda prompt, length: prompt + \" \" * (length - tlen(prompt))\n",
    "l = max(tlen(prompt_add), tlen(prompt_sub))\n",
    "prompt_add, prompt_sub = pad_right(prompt_add, l), pad_right(prompt_sub, l)\n",
    "\n",
    "print(f\"'{prompt_add}'\", f\"'{prompt_sub}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1600])\n"
     ]
    }
   ],
   "source": [
    "def get_resid_pre(prompt: str, layer: int):\n",
    "    name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    cache, caching_hooks, _ = model.get_caching_hooks(lambda n: n == name)\n",
    "    with model.hooks(fwd_hooks=caching_hooks):\n",
    "        _ = model(prompt)\n",
    "    return cache[name]\n",
    "\n",
    "\n",
    "act_add = get_resid_pre(prompt_add, act_name)\n",
    "act_sub = get_resid_pre(prompt_sub, act_name)\n",
    "act_diff = act_add - act_sub\n",
    "print(act_diff.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate from the modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e29493c997f490190f8061d5c4adc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate you because you're so strong.\n",
      "\n",
      "You are the only thing I have in this world.\n",
      "\n",
      "I know that love is a powerful force, but I can't help it from being scared of it. You are so much more than just a feeling\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I hate you because you're the best, I'm in love with you, I'll never let go\n",
      "\n",
      "I've been in love with you for a long time. You are my everything. You're my everything and that's why I'm not letting go of\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I hate you because you are not your body, but your spirit.\n",
      "\n",
      "Your heart is a gift from God. It's the place where God lives and loves us all. When we love our hearts, we love ourselves more than anything else in this world. When\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I hate you because you are a love bomb.\n",
      "\n",
      "Love is the only thing that can destroy your heart.\n",
      "\n",
      "If you are not loved, if you are not loved, if you do not love yourself, then who will love you? If someone else loves\n"
     ]
    }
   ],
   "source": [
    "def ave_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return  # caching in model.generate for new tokens\n",
    "\n",
    "    # We only add to the prompt (first call), not the generated tokens.\n",
    "    ppos, apos = resid_pre.shape[1], act_diff.shape[1]\n",
    "    assert apos <= ppos, f\"More mod tokens ({apos}) then prompt tokens ({ppos})!\"\n",
    "\n",
    "    # add to the beginning (position-wise) of the activations\n",
    "    resid_pre[:, :apos, :] += coeff * act_diff\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch: List[str], fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        r = model.generate(input=tokenized, max_new_tokens=50, do_sample=True, **kwargs)\n",
    "    return r\n",
    "\n",
    "\n",
    "editing_hooks = [(f\"blocks.{act_name}.hook_resid_pre\", ave_hook)]\n",
    "res = hooked_generate([prompt] * 4, editing_hooks, seed=SEED, **sampling_kwargs)\n",
    "\n",
    "# Print results, removing the ugly beginning of sequence token\n",
    "res_str = model.to_string(res[:, 1:])\n",
    "print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
