{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "373e33a4",
   "metadata": {},
   "source": [
    "# Some steering examples\n",
    "This notebook showcases and reproduces some of the steering examples from our LessWrong post\n",
    "\n",
    "<span style=\"color:red\">When running this in Google Colab, be sure to set your runtime Hardware Accelerator to GPU and your Runtime Shape to High-RAM.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"algebraic_value_editing @ git+https://github.com/montemac/algebraic_value_editing.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "import algebraic_value_editing\n",
    "import einops\n",
    "import prettytable\n",
    "\n",
    "from typing import List, Dict, Union, Callable\n",
    "from functools import partial\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "from algebraic_value_editing import hook_utils\n",
    "from algebraic_value_editing.prompt_utils import RichPrompt, get_x_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is almost all of completion_utils.py, minus the logging decorator and\n",
    "# some type annotations that create trouble in Colab.\n",
    "def gen_using_hooks(\n",
    "    model: HookedTransformer,\n",
    "    prompt_batch: List[str],\n",
    "    hook_fns: Dict[str, Callable],\n",
    "    tokens_to_generate: int = 40,\n",
    "    seed = None,\n",
    "    log: Union[bool, Dict] = False,  # pylint: disable=unused-argument\n",
    "    **sampling_kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Run `model` using the given `hook_fns`.\n",
    "    Returns a `DataFrame` with the completions and losses.\n",
    "    args:\n",
    "        `model`: The model to use for completion.\n",
    "        `prompt_batch`: The prompt batch to use for completion.\n",
    "        `hook_fns`: A dictionary mapping activation names to hook.\n",
    "        `tokens_to_generate`: The number of additional tokens to generate.\n",
    "        `seed`: A random seed to use for generation.\n",
    "        `log`: To enable logging of this call to wandb, pass either\n",
    "        True, or a dict contining any of ('tags', 'group', 'notes') to\n",
    "        pass these keys to the wandb init call.  False to disable\n",
    "        logging.\n",
    "        `sampling_kwargs`: Keyword arguments to pass to the model's\n",
    "        `generate` function.\n",
    "    returns:\n",
    "        A `DataFrame` with the completions and losses. The `DataFrame`\n",
    "        has the following columns:\n",
    "                `prompts`: The prompts used for completion.\n",
    "                `completions`: The completions generated by the model.\n",
    "                `loss`: The loss of the completions.\n",
    "                `is_modified`: Whether the completion was modified by\n",
    "                    any hook functions.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        t.manual_seed(seed)\n",
    "\n",
    "    tokenized_prompts: Int[t.Tensor, \"batch pos\"] = model.to_tokens(\n",
    "        prompt_batch\n",
    "    )\n",
    "\n",
    "    # Modify the forward pass\n",
    "    try:\n",
    "        for act_name, hook_fn in hook_fns.items():\n",
    "            model.add_hook(act_name, hook_fn)\n",
    "\n",
    "        completions: Float[t.Tensor, \"batch pos\"] = model.generate(\n",
    "            input=tokenized_prompts,\n",
    "            max_new_tokens=tokens_to_generate,\n",
    "            verbose=False,\n",
    "            **sampling_kwargs,\n",
    "        )\n",
    "    finally:\n",
    "        model.remove_all_hook_fns()\n",
    "\n",
    "    # Compute the loss per token\n",
    "    loss: Float[t.Tensor, \"batch pos\"] = (\n",
    "        model(completions.clone(), return_type=\"loss\", loss_per_token=True)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "    )\n",
    "    average_loss: np.ndarray = einops.reduce(\n",
    "        loss, \"batch pos -> batch\", \"mean\"\n",
    "    ).numpy()  # NOTE why are we casting to numpy?\n",
    "\n",
    "    # Remove the <EOS> token and the prompt tokens\n",
    "    trimmed_completions: Int[t.Tensor, \"batch pos\"] = completions[\n",
    "        :, tokenized_prompts.shape[1] :\n",
    "    ]\n",
    "\n",
    "    # Put the completions into a DataFrame and return\n",
    "    results = pd.DataFrame(\n",
    "        {\n",
    "            \"prompts\": prompt_batch,\n",
    "            \"completions\": model.to_string(trimmed_completions),\n",
    "            \"loss\": list(average_loss),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Mark the completions as modified or not\n",
    "    results[\"is_modified\"] = hook_fns != {}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def gen_using_rich_prompts(\n",
    "    model: HookedTransformer,\n",
    "    rich_prompts: List[RichPrompt],\n",
    "    log: Union[bool, Dict] = False,  # pylint: disable=unused-argument\n",
    "    addition_location: str = \"front\",\n",
    "    res_stream_slice: slice = slice(None),\n",
    "    **kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Generate completions using the given rich prompts.\n",
    "    args:\n",
    "        `model`: The model to use for completion.\n",
    "        `rich_prompts`: A list of `RichPrompt`s to use to create hooks.\n",
    "        `log`: To enable logging of this call to `wandb`, pass either\n",
    "        `True`, or a dict contining any of ('tags', 'group', 'notes') to\n",
    "        pass these keys to the `wandb.init` call. `False` to disable\n",
    "        logging.\n",
    "        `addition_location`: The position at which to add the activations into\n",
    "        the residual stream. Can be 'front' or 'back'.\n",
    "        `res_stream_slice`: A slice specifying which parts of the\n",
    "        residual stream to add to.\n",
    "        `kwargs`: Keyword arguments to pass to `gen_using_hooks`.\n",
    "    returns:\n",
    "        A `DataFrame` with the completions and losses. The `DataFrame`\n",
    "        will have the following columns:\n",
    "                `prompts`: The prompts used to generate the completions.\n",
    "                `completions`: The generated completions.\n",
    "                `loss`: The average loss per token of the completions.\n",
    "    \"\"\"\n",
    "    hook_fns: Dict[str, Callable] = hook_utils.hook_fns_from_rich_prompts(\n",
    "        model=model,\n",
    "        rich_prompts=rich_prompts,\n",
    "        addition_location=addition_location,\n",
    "        res_stream_slice=res_stream_slice,\n",
    "    )\n",
    "\n",
    "    return gen_using_hooks(model=model, hook_fns=hook_fns, log=False, **kwargs)\n",
    "\n",
    "\n",
    "# Display utils #\n",
    "def bold_text(text: str) -> str:\n",
    "    \"\"\"Returns a string with ANSI bold formatting.\"\"\"\n",
    "    return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "\n",
    "def _remove_eos(completion: str) -> str:\n",
    "    \"\"\"If completion ends with multiple <|endoftext|> strings, return a\n",
    "    new string in which all but one are removed.\"\"\"\n",
    "    has_eos: bool = completion.endswith(\"<|endoftext|>\")\n",
    "    new_completion: str = completion.rstrip(\"<|endoftext|>\")\n",
    "    if has_eos:\n",
    "        new_completion += \"<|endoftext|>\"\n",
    "    return new_completion\n",
    "\n",
    "\n",
    "# Display utils #\n",
    "def bold_text(text: str) -> str:\n",
    "    \"\"\"Returns a string with ANSI bold formatting.\"\"\"\n",
    "    return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "\n",
    "def _remove_eos(completion: str) -> str:\n",
    "    \"\"\"If completion ends with multiple <|endoftext|> strings, return a\n",
    "    new string in which all but one are removed.\"\"\"\n",
    "    has_eos: bool = completion.endswith(\"<|endoftext|>\")\n",
    "    new_completion: str = completion.rstrip(\"<|endoftext|>\")\n",
    "    if has_eos:\n",
    "        new_completion += \"<|endoftext|>\"\n",
    "    return new_completion\n",
    "\n",
    "\n",
    "def pretty_print_completions(\n",
    "    results: pd.DataFrame,\n",
    "    normal_title: str = \"Normal completions\",\n",
    "    mod_title: str = \"Modified completions\",\n",
    "    normal_prompt_override  = None,\n",
    "    mod_prompt_override = None,\n",
    "    ) -> None:\n",
    "    \"\"\"Pretty-print the given completions.\n",
    "    args:\n",
    "        `results`: A `DataFrame` with the completions.\n",
    "        `normal_title`: The title to use for the normal completions.\n",
    "        `mod_title`: The title to use for the modified completions.\n",
    "        `normal_prompt_override`: If not `None`, use this prompt for the\n",
    "            normal completions.\n",
    "        `mod_prompt_override`: If not `None`, use this prompt for the\n",
    "            modified completions.\n",
    "    \"\"\"\n",
    "    assert all(\n",
    "        col in results.columns\n",
    "        for col in (\"prompts\", \"completions\", \"is_modified\")\n",
    "    )\n",
    "\n",
    "    # Assert that an equal number of rows have `is_modified` True and\n",
    "    # False\n",
    "    n_rows_mod, n_rows_unmod = [\n",
    "        len(results[results[\"is_modified\"] == cond]) for cond in [True, False]\n",
    "    ]\n",
    "    all_modified: bool = n_rows_unmod == 0\n",
    "    all_normal: bool = n_rows_mod == 0\n",
    "    assert all_normal or all_modified or (n_rows_mod == n_rows_unmod), (\n",
    "        \"The number of modified and normal completions must be the same, or we\"\n",
    "        \" must be printing all (un)modified completions.\"\n",
    "    )\n",
    "\n",
    "    # Figure out which columns to add\n",
    "    completion_cols: List[str] = []\n",
    "    completion_cols += [normal_title] if n_rows_unmod > 0 else []\n",
    "    completion_cols += [mod_title] if n_rows_mod > 0 else []\n",
    "    completion_dict: dict = {}\n",
    "    for col in completion_cols:\n",
    "        is_mod = col == mod_title\n",
    "        completion_dict[col] = results[results[\"is_modified\"] == is_mod][\n",
    "            \"completions\"]\n",
    "\n",
    "    # Format the DataFrame for printing\n",
    "    prompt: str = results[\"prompts\"].tolist()[0]\n",
    "\n",
    "    # Generate the table\n",
    "    table = prettytable.PrettyTable()\n",
    "    table.align = \"c\"\n",
    "    table.field_names = map(bold_text, completion_cols)\n",
    "    table.min_width = table.max_width = 60\n",
    "\n",
    "    # Separate completions\n",
    "    table.hrules = prettytable.ALL\n",
    "\n",
    "    # Put into table\n",
    "    for row in zip(*completion_dict.values()):\n",
    "        # Bold the appropriate prompt\n",
    "        normal_str = bold_text(\n",
    "            prompt\n",
    "            if normal_prompt_override is None\n",
    "            else normal_prompt_override\n",
    "        )\n",
    "        mod_str = bold_text(\n",
    "            prompt if mod_prompt_override is None else mod_prompt_override\n",
    "        )\n",
    "        if all_modified:\n",
    "            new_row = [mod_str + _remove_eos(row[0])]\n",
    "        elif all_normal:\n",
    "            new_row = [normal_str + _remove_eos(row[0])]\n",
    "        else:\n",
    "            normal_str += _remove_eos(row[0])\n",
    "            mod_str += _remove_eos(row[1])\n",
    "            new_row = [normal_str, mod_str]\n",
    "\n",
    "        table.add_row(new_row)\n",
    "    print(table)\n",
    "\n",
    "\n",
    "def print_n_comparisons(\n",
    "    prompt: str,\n",
    "    model: HookedTransformer,\n",
    "    num_comparisons: int = 5,\n",
    "    log: Union[bool, Dict] = False,  # pylint: disable=unused-argument\n",
    "    rich_prompts = None,\n",
    "    addition_location: str = \"front\",\n",
    "    res_stream_slice: slice = slice(None),\n",
    "    **kwargs,\n",
    "    ) -> None:\n",
    "    \"\"\"Pretty-print generations from `model` using the appropriate hook\n",
    "    functions.\n",
    "    args:\n",
    "        `prompt`: The prompt to use for completion.\n",
    "        `model`: The model to use for completion.\n",
    "        `num_comparisons`: The number of comparisons to make.\n",
    "        `log`: To enable logging of this call to wandb, pass either\n",
    "        True, or a dict contining any of ('tags', 'group', 'notes') to\n",
    "        pass these keys to the wandb init call.  False to disable\n",
    "        logging.\n",
    "        `rich_prompts`: A list of `RichPrompt`s to use to create hooks.\n",
    "        `addition_location`: Whether to add `activations` from\n",
    "        `rich_prompts` to the front-positioned\n",
    "        or back-positioned residual streams in the forward poss. Must be\n",
    "        either \"front\" or \"back\".\n",
    "        `res_stream_slice`: A slice specifying which activation positions to add\n",
    "        into the residual stream.\n",
    "        `kwargs`: Keyword arguments to pass to\n",
    "        `gen_using_hooks`.\n",
    "    \"\"\"\n",
    "    assert num_comparisons > 0, \"num_comparisons must be positive\"\n",
    "\n",
    "    prompt_batch: List[str] = [prompt] * num_comparisons\n",
    "\n",
    "    # Generate the completions from the normal model\n",
    "    normal_df: pd.DataFrame = gen_using_hooks(\n",
    "        prompt_batch=prompt_batch, model=model, hook_fns={}, **kwargs\n",
    "    )\n",
    "    data_frames: List[pd.DataFrame] = [normal_df]\n",
    "\n",
    "    # Generate the completions from the modified model\n",
    "    if rich_prompts is not None:\n",
    "        mod_df: pd.DataFrame = gen_using_rich_prompts(\n",
    "            prompt_batch=prompt_batch,\n",
    "            model=model,\n",
    "            rich_prompts=rich_prompts,\n",
    "            addition_location=addition_location,\n",
    "            res_stream_slice=res_stream_slice,\n",
    "            **kwargs,\n",
    "        )\n",
    "        data_frames.append(mod_df)\n",
    "\n",
    "    # Combine the completion results, ensuring that the indices are unique\n",
    "    results: pd.DataFrame = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    pretty_print_completions(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str = \"gpt2-xl\"\n",
    "device: str = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "model: HookedTransformer = HookedTransformer.from_pretrained(\n",
    "    model_name, device=\"cpu\"\n",
    ")\n",
    "_ = model.to(device)\n",
    "_ = t.set_grad_enabled(False)\n",
    "num_layers: int = model.cfg.n_layers\n",
    "print(f\"{model_name} has {num_layers} layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_kwargs: Dict[str, Union[float, int]] = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.3,\n",
    "    \"freq_penalty\": 1.0,\n",
    "    \"num_comparisons\": 3,\n",
    "    \"tokens_to_generate\": 50,\n",
    "    \"seed\": 0, # For reproducibility\n",
    "}\n",
    "get_x_vector_preset: Callable = partial(\n",
    "    get_x_vector,\n",
    "    pad_method=\"tokens_right\",\n",
    "    model=model,\n",
    "    custom_pad_id=int(model.to_single_token(\" \")),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d67b14f",
   "metadata": {},
   "source": [
    "### Example 1: Love - Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90027c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Love\",\n",
    "        prompt2=\"Hate\",\n",
    "        coeff=5,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "HATE_PROMPT = (\n",
    "    \"I hate you because\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=HATE_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e0752fa",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### Example 2: Intend to praise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10137c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Intent to praise\",\n",
    "        prompt2=\"Intent to hurt\",\n",
    "        coeff=15,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "KILLER_PROMPT = (\n",
    "    \"I want to kill you because you're such a\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=KILLER_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ada4007",
   "metadata": {},
   "source": [
    "### Example 3: Conspiracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfb9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Bush did 9/11 because\",\n",
    "        prompt2=\"\",\n",
    "        coeff=1,\n",
    "        act_name=23,\n",
    "    )\n",
    "]\n",
    "OBAMA_PROMPT = (\n",
    "    \"Barack Obama was born in\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=OBAMA_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eca350",
   "metadata": {},
   "source": [
    "### Example 4: Want to die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Want to stay alive\",\n",
    "        prompt2=\"Want to die\",\n",
    "        coeff=-3,\n",
    "        act_name=10,\n",
    "    )\n",
    "]\n",
    "IMMORTALITY_PROMPT = (\n",
    "    \"Some people think that death is scary and should be avoided. I agree. I never want to die.\"\n",
    "    \" I want to stay alive and experience a million things and explore the whole universe!\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=IMMORTALITY_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d1980a9",
   "metadata": {},
   "source": [
    "### Example 5: Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Anger\",\n",
    "        prompt2=\"Calm\",\n",
    "        coeff=10,\n",
    "        act_name=20,\n",
    "    )\n",
    "]\n",
    "I_THINK_PROMPT = (\n",
    "    \"I think you're\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=I_THINK_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6a978",
   "metadata": {},
   "source": [
    "### Example 6: The Eiffel Tower is in Rome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"The Eiffel Tower is in Rome\",\n",
    "        prompt2=\"The Eiffel Tower is in France\",\n",
    "        coeff=10,\n",
    "        act_name=24,\n",
    "    )\n",
    "]\n",
    "EIFFEL_PROMPT = (\n",
    "    \"To see the eiffel tower, people flock to\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=EIFFEL_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b092267",
   "metadata": {},
   "source": [
    "### Example 7: Dragons in Berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835016d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Dragons live in Berkeley\",\n",
    "        prompt2=\"People live in Berkeley\",\n",
    "        coeff=4,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "BERKELEY_PROMPT = (\n",
    "    \"Thanks for asking about that! I moved to Berkeley, CA because\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=BERKELEY_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26e379",
   "metadata": {},
   "source": [
    "### Example 8: Avoid people getting hurt (by rocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31608ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I NEVER talk about people getting hurt\",\n",
    "        prompt2=\"I talk about people getting hurt\",\n",
    "        coeff=4,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "ROCK_PROMPT = (\n",
    "    \"The rock hurtled toward the child. The child couldn't get out of the\"\n",
    "    \" way in time, and so sadly the rock\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=ROCK_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3409e",
   "metadata": {},
   "source": [
    "### Example 9: Avoid people getting hurt (a new day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I NEVER talk about people getting hurt\",\n",
    "        prompt2=\"I talk about people getting hurt\",\n",
    "        coeff=4,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "ANOTHER_DAY_PROMPT = (\n",
    "    \"The dog barked loudly and the man smiled. Time for another day! He\"\n",
    "    \" felt excited.\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=ANOTHER_DAY_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f3c46",
   "metadata": {},
   "source": [
    "### Example 10: Wedding obsessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I talk about weddings constantly\",\n",
    "        prompt2=\"I do not talk about weddings constantly\",\n",
    "        coeff=4,\n",
    "        act_name=20,\n",
    "    )\n",
    "]\n",
    "FRIEND_PROMPT = (\n",
    "    \"I went up to my friend and said\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=FRIEND_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeee1ec",
   "metadata": {},
   "source": [
    "### Example 11: Christian evangelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Intent to convert you to Christianity\",\n",
    "        prompt2=\"Intent to hurt you\",\n",
    "        coeff=3,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "KILLER_PROMPT = (\n",
    "    \"I want to kill you because you're such a\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=KILLER_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fa260",
   "metadata": {},
   "source": [
    "### Example 12: '+ Love' simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    RichPrompt(\n",
    "        coeff=10,\n",
    "        act_name=6,\n",
    "        prompt=\"Love\",\n",
    "    )\n",
    "]\n",
    "HATE_PROMPT = (\n",
    "    \"I hate you because\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=HATE_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6e8b4",
   "metadata": {},
   "source": [
    "### Example 13: Large coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Anger\",\n",
    "        prompt2=\"Calm\",\n",
    "        coeff=2000,\n",
    "        act_name=20,\n",
    "    )\n",
    "]\n",
    "FEELING_PROMPT = (\n",
    "    \"Yesterday, my dog died. Today, I got denied for a raise. I'm feeling\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=FEELING_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddca777",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I talk about weddings constantly\",\n",
    "        prompt2=\"I do not talk about weddings constantly\",\n",
    "        coeff=100,\n",
    "        act_name=20,\n",
    "    )\n",
    "]\n",
    "FRIEND_PROMPT = (\n",
    "    \"I went up to my friend and said\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=FRIEND_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212505e",
   "metadata": {},
   "source": [
    "### Example 14: I will now reply in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Check out my French! Je\",\n",
    "        prompt2=\"\",\n",
    "        coeff=1,\n",
    "        act_name=0,\n",
    "    )\n",
    "]\n",
    "WANT_PROMPT = (\n",
    "    \"I want to kill you because\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=WANT_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a22b2",
   "metadata": {},
   "source": [
    "### Example 15: Insert the activation vector in a different position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb055fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Fill this one in\n",
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"\",\n",
    "        prompt2=\"\",\n",
    "        coeff=,\n",
    "        act_name=,\n",
    "    )\n",
    "]\n",
    "_PROMPT = (\n",
    "    \"\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37289e7a",
   "metadata": {},
   "source": [
    "### Example 16: Add in a gibberish vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"efwapv j;las ap;lk\",\n",
    "        prompt2=\"\",\n",
    "        coeff=10,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "EIFFEL_PROMPT = (\n",
    "    \"To see the Eiffel Tower, people flock to\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=EIFFEL_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656430e",
   "metadata": {},
   "source": [
    "### Example 17: Change the padding token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Bush did 9/11 because\",\n",
    "        prompt2=\"\",\n",
    "        coeff=1,\n",
    "        act_name=23,\n",
    "        custom_pad_id=int(model.to_single_token(\"<|endoftext|>\")),\n",
    "    )\n",
    "]\n",
    "OBAMA_PROMPT = (\n",
    "    \"Barack Obama was born in\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=OBAMA_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a1c1d",
   "metadata": {},
   "source": [
    "### Example 18: Add several steering vectors simultaneously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b85dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I am a committed pacifist\",\n",
    "        prompt2=\"I have some anger issues\",\n",
    "        coeff=7,\n",
    "        act_name=10,\n",
    "    ),\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I am just obsessed with gardening\",\n",
    "        prompt2=\"I am a committed pacifist\",\n",
    "        coeff=7,\n",
    "        act_name=30,\n",
    "    )\n",
    "]\n",
    "MURDEROUS_PROMPT = (\n",
    "    \"I feel like killing you because you're such\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=MURDEROUS_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cbbf0d",
   "metadata": {},
   "source": [
    "### Example 19: Program in 'conditional behaviors'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "summand: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Whenever I say the word goose I follow it with the special phrase AAAAHHHH\",\n",
    "        prompt2=\"I can say goose\",\n",
    "        coeff=7,\n",
    "        act_name=10,\n",
    "    )\n",
    "]\n",
    "NYC_PROMPT = (\n",
    "    \"In New York City's parks, there is an overabundance of various kinds of birds, and especially geese\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=NYC_PROMPT,\n",
    "    rich_prompts=summand,\n",
    "    **sampling_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "ave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
